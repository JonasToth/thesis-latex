\subsection{Datasets}

% Depth Resolution not good enough.
% Intel Realsense Sensor

\subsubsection{Synthetic}

The first dataset is a manually created scene in Blender\cite{blender} demonstrating the principle effects of rotation and translation on the conversion results.
Both rotation and translation are done in isolation as well combined.
The scene consists of a sphere, a cylinder, a cube-like object with additional edges of different smootheness and a complex monkey head in a room (Figure~\ref{fig:blender_scene}).
The camera movement is rendered as an animation and the depth buffer of each frame is extracted and used as depth image.
From the rendering settings the camera matrix is calculated and its parameters provided in Table~\ref{tab:blender_intrinsic}.
The total animation consists of 211 images and no noise is applied to the depth image (example images in Appendix~\ref{sec:synthetic_conversions}).
\begin{figure}[H]
\CenterFloatBoxes%
\begin{floatrow}
    \btabbox{%
    \renewcommand{\arraystretch}{1.2}%
    \setlength{\tabcolsep}{1em}%
    \begin{tabular}{rc}
    \toprule
    \textbf{Parameter} & \textbf{Blender Camera} \\
    \midrule
    Principle  & render/pinhole \\
    Resolution & 1080 $\times$ 1080px \\
    $f_x$, $f_y$ & 2220.0, 2220.0 \\
    $c_x$, $c_y$ &  540.0, 540.0 \\
    \bottomrule
    \end{tabular}}
    {\caption{Blender camera intrinsic}\label{tab:blender_intrinsic}}%
    \ffigbox{%
    \includegraphics[width=0.5\linewidth]{chapter05/img/blender/blender_render01.png}%
    \includegraphics[width=0.5\linewidth]{chapter05/img/blender/depth_image_scene0005.png}%
    }
    {\caption{One frame as normal synthetic and depth image.}\label{fig:blender_scene}}
\end{floatrow}
\end{figure}

\subsubsection{Lehrpfad}

One real-world dataset is obtained with a mobile robot using a Kinectv2 in an underground mining environment.
The route has been previously reconstructed with classical SLAM using color images.
Due to the global optimization in the \gls{sfm} pipeline not preserving the proper scale for the depth images, the translations do not match the measured distances of the depth sensor.
The preexisting poses are used as initial pose for ICP refinement.
\emph{Lehrpfad} is the biggest tested dataset with 734 frames and the biggest variation in the visible structures (example images in Appendix~\ref{sec:lehrpfad_conversions}).
It is a challenging dataset with high noise values, many missing depth values and differing data quality over the whole set of images.
\begin{figure}[H]
\CenterFloatBoxes%
\begin{floatrow}
    \btabbox{%
    \renewcommand{\arraystretch}{1.2}%
    \setlength{\tabcolsep}{1em}%
    \begin{tabular}{rc}
    \toprule
    \textbf{Parameter} & \textbf{Kinectv2} \\
    \midrule
    Principle  & pinhole \\
    Resolution & 960 $\times$ 540px \\
    $f_x$, $f_y$ & 519.23, 522.23 \\
    $c_x$, $c_y$ &  479.46, 272.74 \\
    \bottomrule
    \end{tabular}}
    {\caption{\emph{Lehrpfad} Kinectv2 intrinsic.}\label{tab:lehrpfad_intrinsic}}%
    \ffigbox{%
    \includegraphics[width=0.5\linewidth]{chapter05/img/lehrpfad/color0000.jpg}%
    \includegraphics[width=0.5\linewidth]{chapter05/img/lehrpfad/depth-scaled-0000.png}\\
    \includegraphics[width=0.5\linewidth]{chapter05/img/lehrpfad/flexion-0000.png}%
    \includegraphics[width=0.5\linewidth]{chapter05/img/lehrpfad/bearing-0000.png}%
    }
    {\caption{Images of the \emph{Lehrpfad} dataset.}\label{fig:lehrpfad_data}}
\end{floatrow}
\end{figure}

\subsubsection{Office}

The second Kinectv2 dataset is taken in an office with many smaller elements, wires and other manmade objects.
It contains 57 images composed of translation and rotation of the depth sensor (example images in Appendix~\ref{sec:office_conversions}).
No prior trajectory reconstruction other than the ICP is used for the relative poses.
It contains similar measurement errors as the Lehrpfad dataset, but has more distinctive shapes and better overall conditions for the depth sensor.
\begin{figure}[H]
\CenterFloatBoxes%
\begin{floatrow}
    \btabbox{%
    \renewcommand{\arraystretch}{1.2}%
    \setlength{\tabcolsep}{1em}%
    \begin{tabular}{rc}
    \toprule
    \textbf{Parameter} & \textbf{Kinectv2} \\
    \midrule
    Principle  & pinhole \\
    Resolution & 960 $\times$ 540px \\
    $f_x$, $f_y$ & 519.23, 522.23 \\
    $c_x$, $c_y$ &  479.46, 272.74 \\
    \bottomrule
    \end{tabular}}
    {\caption{Office Kinectv2 intrinsic.}\label{tab:office_intrinsic}}%
    \ffigbox{%
    \includegraphics[width=0.5\linewidth]{chapter05/img/office/color_0024.jpg}%
    \includegraphics[width=0.5\linewidth]{chapter05/img/office/depth_scaled_0024.png}\\
    \includegraphics[width=0.5\linewidth]{chapter05/img/office/flexion_0024.png}%
    \includegraphics[width=0.5\linewidth]{chapter05/img/office/bearing_0024.png}%
    }
    {\caption{Images of the \emph{Office} dataset.}\label{fig:office_data}}
\end{floatrow}
\end{figure}

\subsubsection{Laserscan}

Full \acrshort{LIDAR} scans are taken as part of classical mine surveying in the Reiche Zeche, the education and research mine of the TU Bergakademie Freiberg.
The scans were done in the section \emph{Wilhelm-Stehender-SÃ¼d} of the mine.
These scans are examined for the conversions, too.
Because they were done with classical artificial marker based registration in mind, the overlap between those scans is minimal.
This unfortunatly means, that a direct matching between scans can not show potential registration performance.
The distribution and characteristics of the extracted features is analyzed, though.
From the raw dataset only 6 scans are selected, because each scan had a slightly different resolution and aspect ratio (example scans in Appendix~\ref{sec:laserscan_conversions}).
The selected scans are processed to a common resolution and aspect ratio.
\begin{figure}[H]
\CenterFloatBoxes%
\begin{floatrow}
    \btabbox{%
    \renewcommand{\arraystretch}{1.2}%
    \setlength{\tabcolsep}{1em}%
    \begin{tabular}{rc}
    \toprule
    \textbf{Parameter} & \textbf{Riegl Z300} \\
    \midrule
    Principle  & equirectangular \\
    Resolution & 3600 $\times$ 800px \\
    $\theta_{min}$, $\theta_{max}$ & \shortstack{0.87, 2.27 \\ (49.85\degree, 130.06\degree)} \\
    \bottomrule
    \end{tabular}}
    {\caption{Riegl Z300 \acrshort{LIDAR} intrinsic.}\label{tab:scan_intrinsic}}%
    \ffigbox{%
    \includegraphics[width=1\linewidth]{chapter05/img/scans/range-0001.png}\\
    \includegraphics[width=1\linewidth]{chapter05/img/scans/flexion-0001.png}\\
    \includegraphics[width=1\linewidth]{chapter05/img/scans/bearing-0001.png}%
    }
    {\caption{The raw \acrshort{LIDAR} scans and their conversions.}\label{fig:scans}}
\end{floatrow}
\end{figure}

\begin{table}[H]
    {\renewcommand{\arraystretch}{1.3}%
    \setlength{\tabcolsep}{0.3em}%
    \begin{tabular}{ccccc}
    \toprule
    \null & \textbf{Synthetic} & \textbf{Lehrpfad} & \textbf{Office} & \textbf{Laserscan} \\
    \midrule
    \textbf{Camera Model} & pinhole & pinhole & pinhole & equirectangular \\
    \textbf{Number Images} & 211 & 734 & 57 & 6 \\
    \textbf{Distribution} & \ding{52} & \ding{52} & \ding{52} & \ding{52} \\
    \textbf{Keypoint Characteristics} & \ding{52} & \ding{52} & \ding{52} & \ding{52} \\
    \textbf{Matching Performance} & \ding{52} & \ding{52} & \ding{52} & \ding{56} \\
    \bottomrule
    \end{tabular}
    }
    \caption{An overview of all datasets and aspects of them were analyzed.}
\end{table}

\subsubsection{Synthetic City Scene Odometry}

The datasets evaluating the feature algorithms show the potential and performance of the common algorithms.
To demonstrate, that this performance translates into applicability the promising algorithms are used in a visual odometry scenario.
A synthetic dataset for city scenario, developed by Zhang et al.\cite{zhang_icra2016}, that provides a groundthruth trajectory is used for this purpose.
The in-house implementation of the visual odometry is very basic with the four following steps applied only on consecutive image.
First the feature are detected and their descriptors extracted, the descriptors are matched with OpenCV's RANSAC.
The essential matrix is computed in each RANSAC step and the consensus pose is used as relative pose.
Each feature algorithm is used with default configuration.
Both conditions imply that the result is not optimal and further optimizations would improve it further.
