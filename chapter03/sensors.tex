\subsection{Depth Sensors}

Depth sensors play an increasingly important role in modern robotic applications.
They give robots a fast mean to detect obstacles and locate themselves in their environments.
The following paragraphs describe the underlying sensing principles for the depth sensors commonly used in mobile robotic applications.

\subsubsection{Structured-Light Depth Sensors}

Structured-light depth sensors project a known light pattern into space and optically sense the reflection\cite{blais_2003}.
The pattern appears distorted from a viewpoint different than the projector due to the shape of the reflecting object.
Figure~\ref{fig:sl_face} provides an example for structured-light sensing using visible light.
Stripes and grids of lines or points are common for sensors in robotic applications\cite{blais_2003}, such as the Kinectv1\cite{wasenmuller_accv2016} and Intel RealSense\cite{intel_realsense} sensors.
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{chapter03/img/depth_pattern_face.png}
        \caption{Visible structured light pattern}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{chapter03/img/depth_face_reconstructed.png}
        \caption{Reconstructed result}
    \end{subfigure}
    \caption[Demonstration of structured-light depth sensors]{\emph{Demonstration of structured-light depth sensors.} One way to reconstruct depth information are structured light depth sensors. A predefined light pattern is projected into space. As the photons are reflected from obstacles at different distances relative to the observer, the light pattern appears transformed. This deformation is used to reconstruct the geometry of the objects. The light emitted is usually not visible to humans as infrared light sources and cameras are commonly used in robotic applications. The images are taken from \cite{sl_depthsensor_calibration}.}\label{fig:sl_face}
\end{figure}
Viewing the projected patterns from multiple viewpoints allows triangulation to recover the depth information.
Mobile robotic depth sensors achieve frame rates of 30\,\acrshort{fps} or more.
The projected pattern should not interfere with other optic devices leading to the usage of infrared light.

\subsubsection{\acrlong{ToF} (\acrshort{ToF}) cameras}

The second prevalent sensing technology measures the traveling time of light\cite[p. 27-41]{hansard_springer2012}.
Multiplying the measured round-trip time by the known speed of light in the surrounding medium results in twice the distance of the object.
Such a sensor system requires a light sources.
Unlike the specialized patterns emitted for structured light, this can be an ordinary infrared \acrshort{led}, again preventing interference in other spectra of light.
\begin{figure}[b!]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{chapter03/img/tof_traveltime_original.png}
        \caption{One way to measure the time-of-flight for photons is to emit a light pulse and measure the round-trip time until the reception of the reflection.}\label{fig:tof_roundtrip}
    \end{subfigure}\quad
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{chapter03/img/tof_phase_shift_original.png}
        \caption{The second, more recent approach is measuring the phase-shift of photons relative to the light source. This allows the usage of a continuous light source.}\label{fig:tof_phase_shift}
    \end{subfigure}
    \caption[Illustration of the measuring principles for \acrshort{ToF} cameras]{\emph{Illustration of the measuring principles for \acrshort{ToF} cameras.} \acrlong{ToF} is first of all a measurement principle for distances used in different contexts. In this specific case \acrshort{ToF} sensors measure the round-trip time of light from emission, reflection and sensing. Multiplying the measured time by the speed of light results in the distance the light traveled. The illustrations are adopted from\cite{tof_cameras}.}\label{fig:tof_illustration}
\end{figure}
A pulsed and synchronized light emitter, as exemplified in Figure~\ref{fig:tof_roundtrip}, allows direct measurement of the round-trip time of light.
Such a system requires exact synchronisation and a high temporal resolution to achieve accurate measurements.
The different and more recent development in depth sensing technology is measuring the phase shift (Figure~\ref{fig:tof_phase_shift}) of the reflected light to recover the traveling time of the light wave.
This is the underlying principle of the Kinectv2 depth sensor\cite{wasenmuller_accv2016}.
The drawback of this technology is the limited range of the depth sensor, due to the ambiguity of the wave signal.
Using modulation techniques to create a wave signal with more than one frequency increases the range of the sensor\cite[p. 27-41]{hansard_springer2012}.

\subsubsection{\acrlong{LIDAR} (\acrshort{LIDAR})}

\acrlong{LIDAR} (\acrshort{LIDAR}) uses the same measurement principle as \acrshort{ToF} cameras\cite[p. 239]{taylor_crc2019}.
It determines the distance of an object using round-trip time measurements.
The difference between \acrshort{LIDAR} and \acrshort{ToF} cameras is the usage of a \acrshort{laser} as light source and the overall setup of the sensor system.
The most common configurations use a horizontally rotating mirror that reflects the beam into the room.
For vertical coverage, the measurement head is rotated in total.
Each complete horizontal turn is called a scan line.

Some \acrshort{LIDAR}s, especially mobile systems with real time requirements, measure tens of scan lines.
Such systems achieve a higher frame rate.
A dense \acrshort{LIDAR} scan is required for the optical feature-based registration approach.
Such scans can be created with terrestrial \acrshort{LIDAR} stations, common in geodesic applications.
Additionally, the intensity of the reflected light beam might be obtained as second channel.

Flash \acrshort{LIDAR} is a scanner-less type of \acrshort{LIDAR} that does not require a rotating mirror or other mechanical system\cite{natale_ssiai2010}.
A flash \acrshort{LIDAR} uses the measurement principles described for \acrshort{ToF} cameras and measures a dense depth image in one shot.
