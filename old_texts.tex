\section{Old Texts}
\subsection{Goals}

Goals are registering pointclouds

\begin{figure}[H]
    \input{images/full-flow.tikz}
	\caption[Flowchart of final processing pipeline]{The final processing pipeline is able to register depth images in existing pointclouds with much higher resolution}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/collage_v3.png}
    \caption{Overview of possible Feature-Images from a depth image.}
\end{figure}

\subsection{Reference Data Processing}

\begin{figure}[H]
	\includegraphics[width=\linewidth]{images/bild1.png}
	\caption[Laserscan as equirectangular depth image]{This image shows a laserscan that is converted into an equirectangular depth image}
\end{figure}

\subsection{Feature Matching}

\begin{figure}[H]
	\includegraphics[width=\linewidth]{images/match-result.png}
	\caption[Examplaric Image Matching]{Matching two flexion images is possible, the results are mixed though. This match is done with ORB}
\end{figure}

\subsection{Potential Deviations from Plan}

\subsubsection*{Feature Matching Considerations}
\begin{itemize}
    \item depth-map preprocessing to fill holes and improve their quality
    \item smoothing or either depth-maps or feature images
    \item pyramid approach for multi-resolution feature images (similar to SIFT)
    \item change offset of neighbour pixels (e.g.~four pixel difference in each direction instead of just one)
    \item concentrate more on deep and wide evaluation of feature matching instead of full localization pipeline as a more robust foundation for future work
    \item just try out an out-of-the-box SfM-solution
\end{itemize}

\section{Introduction}

The dominant method to register pointclouds with pointclouds or depth
images is the use of a variant of the \gls{icp}\cite{Besl1992}
algorithm. Pomerleau et.al\cite{Pomerleau2015} provide a review of pointcloud registration methods.
Even though there are many different variations of \gls{icp}, it has
some common problems. The algorithm requires a good initial
transformation. Pointclouds with order-of-magnitude different
resolutions can produce unstable results. Convergence might require many
iterative steps and can not be predicted. Each of these iterative steps
is computationally expensive and does not scale very well to massive
datasets.

All these apsects give opportunity for a better solution to the problem
of registering depth images, e.g.~from the Kinect-v2, to existing
high-resolution pointclouds.
Inspired by Scaramuzza's \Glspl{bearing-angle-image}\cite{Scaramuzza2007}, Lin et.al\cite{Lin2017} apply the classical feature detector \gls{surf}\cite{Bay2006} on \Glspl{bearing-angle-image} and are able to register pointclouds to each other.
This work improves upon this idea in multiple ways.
\Glspl{bearing-angle-image} are not rotation invariant as they encode local geometry only in one direction.
Neither are they viewpoint invariant, as the \gls{bearing-angle} changes when the sensing lightray hits the same surface from a different angle.
In an attempt to overcome these limitations this work proposes new derived images that are rotation and viewpoint invariant, with the most promising variant being \Glspl{flexion-image}.

\section{Approach}\label{approach}

\subsection{Sensor description}

\begin{itemize}
    \item Determine Angular resolution for two light-rays, Laserscans are known and image sensors require intrinsic
    \item for Kinect, simplified pinhole model without distortion is assumed, rectification can be done beforehand
    \item resulting pictures are depth maps, that need conversion to euclidian distances
    \item Depth-Maps need to be converted into range data
\end{itemize}

\begin{figure}[H]
    \input{images/depth-map.tikz}
    \caption[Range Data and Depth Maps visualized]{Depthimages encode the othorgraphic depth. For the following calculations the conversion to the euclidian depth is necessary as first preprocessing step.}
\end{figure}

Projecting pixels onto the unit sphere.
\begin{align}
	\begin{pmatrix}x \\ y \\ z \end{pmatrix} &= \begin{pmatrix} \frac{u - c_x}{f_x} \\ \frac{v - c_y}{f_y} \\ 1 \end{pmatrix} \\
	\begin{pmatrix}x_s \\ y_s \\ z_s \end{pmatrix} &= \frac{\sqrt{1 + x^2 + y^2}}{1 + x^2 + y^2} \begin{pmatrix} x \\ y \\ z \end{pmatrix}
\end{align}

\begin{itemize}
    \item Angular resolution between two pixels needs to be calculated for the camera
    \item intrinsic calibration required, but can be cached statically afterwards
\end{itemize}

\begin{figure}[H]
    \input{images/unit-sphere-model.tikz}
	\caption[Angle between two pixels in the pinhole model]{The angle between two pixels of a rectified pinhole image.}
\end{figure}
To determine the angle spanned between two corresponding lightrays for two pixels the pixel coordinate needs to be backprojected to the unit sphere.
\begin{align}
    \begin{pmatrix} u \\ v \end{pmatrix} \mapsto \begin{pmatrix} x_s \\ y_s \\ z_s \end{pmatrix}
\end{align}
The resulting vectors have unit length, simplifying the angle-calculation.
\begin{align}
    \vec{r_1} \cdot \vec{r_2} &= \abs{\vec{r_1}} \abs{\vec{r_2}} \cos \Delta\varphi \\
    \cos \Delta\varphi &= x_{s,1} x_{s,2} + y_{s,1} y_{s,2} + z_{s,1} z_{s,2} \\
    \Delta\varphi &= \arccos{x_{s,1} x_{s,2} + y_{s,1} y_{s,2} + z_{s,1} z_{s,2}}
\end{align}
The angle $\Delta\varphi$ is not a constant for the pinhole model for different pixels but can be calculated once for each intrinsic.

\subsection{Preprocessing of range Data}\label{preprocessing-of-range-data}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/collage_v3.png}
    \caption{Overview of possible Feature-Images from a depth image.}
\end{figure}

Instead of registering the depth images based on single points as \gls{icp} algorithms do, this thesis develops a new framework to use existing image features like \Gls{sift} and \Gls{surf} to calculate the transformation between pointcloud and depth image.
Both, the pointcloud and the depth image, are first converted to a gray-scale feature image.
Local geometric structure is encoded through the visual structure of the feature image and is detected and matched via a classical matching pipeline.

In general depth images do not contain a valid value for each pixel.
The Kinectv2 returns $0$ if no measurement was possible.
Whenever such a value would be included in the calculation of the derived quantities the value is set to $0$ as well.

Each feature quantity has a different range of valid values.
To create visually distinct images these values are scaled and quantized to the image depth.
With $p$ the final value for the pixel, $t_{min}$ and $t_{max}$ the value range for the target image depth and $v$ the scalar value of the feature quantity, the conversion is done with the following formula.
\begin{align}
    p = \floor[\Bigg]{\frac{{(t_{max} - t_{min})} {(v - t_{min})}}{t_{max} - t_{min}} + t_{min}}
\end{align}
Depth images themself are usually stored as $16~bit~unsigned$ grayscale images.
For feature images with the same image depth $t_{min} = 0$ and $t_{max} = 65535$

\subsection{Other derived feature quantities}

Other attempts to create feature images are described in the thesis as well.
The performance in feature detection and matching is compared.
From visual inspection the author expects \Glspl{flexion-image} to perform best though.


All of those experiments are implemented and compared in the thesis.

\subsection{Transformation calculation}\label{transformation-calculation}

Starting with the feature image, a visual localization pipeline is
established that works solely on omnidirectional data, thus is general
enough to utilize a wide range of sensors and camera models.

The full workflow of the pipeline is as follows:

\begin{enumerate}
\item Omnidirectional images are mapped onto a cubemap to reduce distortion,
  the feature images are calculated for both the data to localize,
  e.g.~Kinect depth image and the reference data, e.g.~terrestrial
  laserscans.
\item Visual Features are detected and matched between the sensor to
  localize and the reference data.
\item Classical RANSAC performs the stable calculation of the
  Essential matrix that is then decomposed into rotation and translation.
\item The resulting rotation and translation is optimized. The objective
  function is the distance of the detected features to the epipolar
  lines.
\item The unscaled translation is scaled with the depth data from the sensor
  input. This scaling can be done both with the input depth sensor and
  the reference data. The resulting difference is again subject to
  optimization.
\end{enumerate}

The result is the pose of the camera relative to the registered image as well as an error of the pose.

\section{Novelity}\label{novelity}

The idea to use optical features for multimodal sensor registration is
not new and goes back to Scaramuzza's approach to calibrate a
laserscanner to an optical camera. To the best knowledge of the author
only bearing angle images were used though.

Bearing angle images do come with some issues. They encode only the
relationship of two neighbouring points. Therefore, they are not
invariant to rotation. It is possible to calculate the bearing angle in
all eight bearing directions (horizontal, vertical, diagonal, antidiagonal in
both directions). This results to higher computational costs, especially
for the feature detection and matching pipeline.

This is the reason this work proposes different feature images that all
encode local geometry as a scalar value, gaussian curvature, mean
curvate and flexion as described above. These feature images are
compared to bearing angles. Flexion images are expected to perform the best
as they give the best visual structure.

If the proposed localization pipeline does work as wished it gives a new
intermodal approach to localization and visual odometry.

Futhermore, it allows to apply algorithms and approaches from the
classical visual feature-world to depth sensors that became widely
available in robotic operations.
