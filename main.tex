\documentclass[doktyp=marbeit,fontsize=12pt,sprache=english,draft=true,hausschrift=true,fleqn]{TUBAFarbeiten}

% packages
\usepackage{amsmath}
\usepackage[ngerman]{babel}
\usepackage{blindtext}
\usepackage{caption}
\usepackage{calc}
\usepackage{cite}
\usepackage{enumitem}
\usepackage[T1]{fontenc}
\usepackage{floatrow} % rows of table and pictures
\usepackage{graphicx}
\usepackage{gensymb}
\usepackage[utf8]{inputenc}
\usepackage{makeidx}
\usepackage[fleqn]{mathtools} % mathtools und links buendig machen
\usepackage{subcaption}

% Images with mathcha.io 
\usepackage{tikz}
\usetikzlibrary{fadings}
\usepackage{mathdots}
\usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{booktabs}
% Images end

\usepackage{subscript} % tief stellen
%\usepackage[square]{natbib}
\usepackage{setspace}
\renewcommand{\baselinestretch}{1.5}

\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\abs}{\lVert}{\rVert} % definiere absoluten betrag

\usepackage[acronym,toc]{glossaries}
\glstoctrue%

\newglossary[nlg]{symbols}{nls}{nlo}{Symbol Definition}
\makeglossaries%
\loadglsentries{glossary}
\loadglsentries{symbols}

% tubaf zeugs
\input{thesis-meta}

\setcounter{tocdepth}{3}
%\setcounter{secnumdepth}{3}

% \makeindex

% start the content
\begin{document}

\maketitle

\TUBAFErklaerungsseite%
\tableofcontents
\newpage

\printglossary[type=\acronymtype]%
\newpage

\printglossary[type=symbols]%
\glsaddallunused[symbols]
\newpage

\input{structure}

\newpage
\newpage
\newpage

\section{Old Texts}
\subsection{Goals}

Goals are registering pointclouds

\begin{figure}[H]
    \input{images/full-flow.tikz}
	\caption[Flowchart of final processing pipeline]{The final processing pipeline is able to register depth images in existing pointclouds with much higher resolution}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/collage_v3.png}
    \caption{Overview of possible Feature-Images from a depth image.}
\end{figure}

\subsection{Reference Data Processing}

\begin{figure}[H]
	\includegraphics[width=\linewidth]{images/bild1.png}
	\caption[Laserscan as equirectangular depth image]{This image shows a laserscan that is converted into an equirectangular depth image}
\end{figure}

\subsection{Feature Matching}

\begin{figure}[H]
	\includegraphics[width=\linewidth]{images/match-result.png}
	\caption[Examplaric Image Matching]{Matching two flexion images is possible, the results are mixed though. This match is done with ORB}
\end{figure}

\subsection{Potential Deviations from Plan}

\subsubsection*{Feature Matching Considerations}
\begin{itemize}
    \item depth-map preprocessing to fill holes and improve their quality
    \item smoothing or either depth-maps or feature images
    \item pyramid approach for multi-resolution feature images (similar to SIFT)
    \item change offset of neighbour pixels (e.g.~four pixel difference in each direction instead of just one)
    \item concentrate more on deep and wide evaluation of feature matching instead of full localization pipeline as a more robust foundation for future work
    \item just try out an out-of-the-box SfM-solution
\end{itemize}


\section{Introduction}

The dominant method to register pointclouds with pointclouds or depth
images is the use of a variant of the \gls{icp}\cite{Besl1992}
algorithm. Pomerleau et.al\cite{Pomerleau2015} provide a review of pointcloud registration methods.
Even though there are many different variations of \gls{icp}, it has
some common problems. The algorithm requires a good initial
transformation. Pointclouds with order-of-magnitude different
resolutions can produce unstable results. Convergence might require many
iterative steps and can not be predicted. Each of these iterative steps
is computationally expensive and does not scale very well to massive
datasets.

All these apsects give opportunity for a better solution to the problem
of registering depth images, e.g.~from the Kinect-v2, to existing
high-resolution pointclouds.
Inspired by Scaramuzza's \Glspl{bearing-angle-image}\cite{Scaramuzza2007}, Lin et.al\cite{Lin2017} apply the classical feature detector \gls{surf}\cite{Bay2006} on \Glspl{bearing-angle-image} and are able to register pointclouds to each other.
This work improves upon this idea in multiple ways.
\Glspl{bearing-angle-image} are not rotation invariant as they encode local geometry only in one direction.
Neither are they viewpoint invariant, as the \gls{bearing-angle} changes when the sensing lightray hits the same surface from a different angle.
In an attempt to overcome these limitations this work proposes new derived images that are rotation and viewpoint invariant, with the most promising variant being \Glspl{flexion-image}.

\section{Approach}\label{approach}

\subsection{Sensor description}

\begin{itemize}
    \item Determine Angular resolution for two light-rays, Laserscans are known and image sensors require intrinsic
    \item for Kinect, simplified pinhole model without distortion is assumed, rectification can be done beforehand
    \item resulting pictures are depth maps, that need conversion to euclidian distances
    \item Depth-Maps need to be converted into range data
\end{itemize}

\begin{figure}[H]
    \input{images/depth-map.tikz}
    \caption[Range Data and Depth Maps visualized]{Depthimages encode the othorgraphic depth. For the following calculations the conversion to the euclidian depth is necessary as first preprocessing step.}
\end{figure}

Projecting pixels onto the unit sphere.
\begin{align}
	\begin{pmatrix}x \\ y \\ z \end{pmatrix} &= \begin{pmatrix} \frac{u - c_x}{f_x} \\ \frac{v - c_y}{f_y} \\ 1 \end{pmatrix} \\
	\begin{pmatrix}x_s \\ y_s \\ z_s \end{pmatrix} &= \frac{\sqrt{1 + x^2 + y^2}}{1 + x^2 + y^2} \begin{pmatrix} x \\ y \\ z \end{pmatrix}
\end{align}

\begin{itemize}
    \item Angular resolution between two pixels needs to be calculated for the camera
    \item intrinsic calibration required, but can be cached statically afterwards
\end{itemize}

\begin{figure}[H]
    \input{images/unit-sphere-model.tikz}
	\caption[Angle between two pixels in the pinhole model]{The angle between two pixels of a rectified pinhole image.}
\end{figure}
To determine the angle spanned between two corresponding lightrays for two pixels the pixel coordinate needs to be backprojected to the unit sphere.
\begin{align}
    \begin{pmatrix} u \\ v \end{pmatrix} \mapsto \begin{pmatrix} x_s \\ y_s \\ z_s \end{pmatrix}
\end{align}
The resulting vectors have unit length, simplifying the angle-calculation.
\begin{align}
    \vec{r_1} \cdot \vec{r_2} &= \abs{\vec{r_1}} \abs{\vec{r_2}} \cos \Delta\varphi \\
    \cos \Delta\varphi &= x_{s,1} x_{s,2} + y_{s,1} y_{s,2} + z_{s,1} z_{s,2} \\
    \Delta\varphi &= \arccos{x_{s,1} x_{s,2} + y_{s,1} y_{s,2} + z_{s,1} z_{s,2}}
\end{align}
The angle $\Delta\varphi$ is not a constant for the pinhole model for different pixels but can be calculated once for each intrinsic.

\subsection{Preprocessing of range Data}\label{preprocessing-of-range-data}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/collage_v3.png}
    \caption{Overview of possible Feature-Images from a depth image.}
\end{figure}

Instead of registering the depth images based on single points as \gls{icp} algorithms do, this thesis develops a new framework to use existing image features like \Gls{sift} and \Gls{surf} to calculate the transformation between pointcloud and depth image.
Both, the pointcloud and the depth image, are first converted to a gray-scale feature image.
Local geometric structure is encoded through the visual structure of the feature image and is detected and matched via a classical matching pipeline.

In general depth images do not contain a valid value for each pixel.
The Kinectv2 returns $0$ if no measurement was possible.
Whenever such a value would be included in the calculation of the derived quantities the value is set to $0$ as well.

Each feature quantity has a different range of valid values.
To create visually distinct images these values are scaled and quantized to the image depth.
With $p$ the final value for the pixel, $t_{min}$ and $t_{max}$ the value range for the target image depth and $v$ the scalar value of the feature quantity, the conversion is done with the following formula.
\begin{align}
    p = \floor[\Bigg]{\frac{{(t_{max} - t_{min})} {(v - t_{min})}}{t_{max} - t_{min}} + t_{min}}
\end{align}
Depth images themself are usually stored as $16~bit~unsigned$ grayscale images.
For feature images with the same image depth $t_{min} = 0$ and $t_{max} = 65535$

\subsection{\Glspl{bearing-angle-image}}

\begin{figure}[H]
    \centering
    \input{images/bearing_angle.tikz}%
    \caption[Schematic Representation of Bearing-Angles]{This figure shows the relationship of the light rays that form the \gls{bearing-angle}.}
\end{figure}

Existing literature\cite{Scaramuzza2007,Lin2017} proposes \Glspl{bearing-angle-image} were each pixel is the angle between the current point, the optical center and the previous point.
The neighbourhood relationship can be choosen arbitrarily resulting in four first-order \Glspl{bearing-angle-image}, horizontal, vertical, diagonal and antidiagonal.
The second variable is the direction the angle is calculted, e.g.~for horizontal images it can be calculated from left-to-right or right-to-left.
This does not exhibit new information, because the angle of the other direction is immediatly known from the fact that the sum of the angles is $180\degree$.
Nontheless, the direction must be defined to obtain stable visual features.

The formula for the \gls{bearing-angle} $\beta$ is derived with the cosine theorem.
For the horizontal left-to-right calculation the formula is as follows.
\begin{align}
    \beta&= \arccos%
            \frac{d_{i,j} - d_{i-1,j} \cos \Delta\varphi}%
                 {\sqrt{d_{i,j}^2 + d_{i-1,j}^2 - 2 d_{i,j} d_{i-1,j} \cos \Delta\varphi}}
\end{align}
Using a different direction or other neighbourhood relation the indices for the depth values need to be changed and a different angular resolution needs to be calculated.
The \Gls{bearing-angle} is in the range $(0, \pi)~rad$ and gets scaled accordingly.

\subsection{\Glspl{flexion-image}}\label{flexion-image-section}

Each pixel of a \Gls{flexion-image} is the dot-product of the local normal
calculated from horizontal and vertical neighbouring pixel with the
normal calculated with the diagonal neighbours.
Figure~\ref{fig:flexion-image-scetched} demonstrates the disparity of both normals for an arbitrary surface patch.
The normal calculated from the diagonal and vertical neighbouring vertices are drawn in blue and the normal calculated with the diagonals red.
Both normal-vectors are drawn with their origin in the central depth value.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{images/scetch_flexion.png}
    \caption[Schematic Representation of Flexion]{This figure demonstrates how flexed surfaces have different normals for diagonal and non-diagonal estimation. This difference is utilized as measure for flexion.}%
    \label{fig:flexion-image-scetched}
\end{figure}

The flexion $f$ is defined as
\begin{align}
    f &= \abs{n_1 \cdotp n_2}
\end{align}
Because $n_1$ and $n_2$ are of length $1$ the value of $f$ is in the range $[0, 1]$ and gets scaled accordingly.

The smaller the dot-product gets, the higher is the local flexion of the
surface. This local property of the geometry then results in visual
features detectable with classical feature detectors and descriptors like
\Gls{sift} or \Gls{surf}.

\subsection{Other derived feature quantities}

Other attempts to create feature images are described in the thesis as well.
The performance in feature detection and matching is compared.
From visual inspection the author expects \Glspl{flexion-image} to perform best though.

The \gls{max-curve-image} tries generalize the \gls{bearing-angle} to be rotation invariant as it takes the \gls{bearing-angle} in each direction into account.
Additionally to the left-sided \gls{bearing-angle} the right-sided angle is calculated as well and finally added.

\begin{figure}
    \input{images/max-curve.tikz}
    \caption[Schematic Representation of the Max-Curve]{The Max-Curve composes two \Glspl{bearing-angle} in vertical, horizontal, diagonal and antidiagonal direction. The maximum angle is then selected as pixel value.}
\end{figure}

This makes the measure more robust to rotation, but does not produce good features.
\begin{align}
    B_{max} &= \max{\{B_{diagonal}, B_{antidiagonal}, B_{horizontal}, B_{vertical}\}}
\end{align}

As a different analytical approach to producing feature images the calculation of the \gls{curvature} for each depth value.
The two common measures of curvature in differential geometry are \gls{gaussian-curvature} and \gls{mean-curvature}\cite{Kuhnel2008}.

If the function is known as a graph the \Gls{gaussian-curvature} can be estimated using the derivatives of the function.
For depth data each depth value is a sample of this function graph and numeric approximation of the derivatives allows the calculation of the curvature.
\begin{align}
    \mathfrak{K} &= \frac{f_{uu} f_{vv} - f_{uv}^2}{{(1 + f_u^2 + f_v^2)}^2}
\end{align}
With
\begin{align*}
    f_{x} &= \frac{y_1 - y_0}{\Delta x} \\
    f_{xx} &= \frac{y_1 + y_{-1} - 2 y_0}{{\Delta x}^2}
\end{align*}
as approximation for the derivatives.

Similarly, the \Gls{mean-curvature} can be calculated with a different formula.
\begin{align}
    \mathfrak{H} &= \frac{{(1 + f_{v}^2)} f_{uu} - 2 f_u f_v f_{uv} + {(1 + f_u^2)} f_{vv}}{2 \sqrt{1 + f_u^2 + f_v^2}^3}
\end{align}
Both measures of curvature are $\mathfrak{K},\mathfrak{H} \in {\rm I\!R}$.
To convert them into a meaningful grayscale image they need to be clamped to arbitrary bounds, that can be choosen based on visual distinctiveness or other heuristics.
After clamping the values are scaled and quantized accordingly.

All of those experiments are implemented and compared in the thesis.

\subsection{Transformation calculation}\label{transformation-calculation}

Starting with the feature image, a visual localization pipeline is
established that works solely on omnidirectional data, thus is general
enough to utilize a wide range of sensors and camera models.

The full workflow of the pipeline is as follows:

\begin{enumerate}
\item Omnidirectional images are mapped onto a cubemap to reduce distortion,
  the feature images are calculated for both the data to localize,
  e.g.~Kinect depth image and the reference data, e.g.~terrestrial
  laserscans.
\item Visual Features are detected and matched between the sensor to
  localize and the reference data.
\item Classical RANSAC performs the stable calculation of the
  Essential matrix that is then decomposed into rotation and translation.
\item The resulting rotation and translation is optimized. The objective
  function is the distance of the detected features to the epipolar
  lines.
\item The unscaled translation is scaled with the depth data from the sensor
  input. This scaling can be done both with the input depth sensor and
  the reference data. The resulting difference is again subject to
  optimization.
\end{enumerate}

The result is the pose of the camera relative to the registered image as well as an error of the pose.

\section{Novelity}\label{novelity}

The idea to use optical features for multimodal sensor registration is
not new and goes back to Scaramuzza's approach to calibrate a
laserscanner to an optical camera. To the best knowledge of the author
only bearing angle images were used though.

Bearing angle images do come with some issues. They encode only the
relationship of two neighbouring points. Therefore, they are not
invariant to rotation. It is possible to calculate the bearing angle in
all eight bearing directions (horizontal, vertical, diagonal, antidiagonal in
both directions). This results to higher computational costs, especially
for the feature detection and matching pipeline.

This is the reason this work proposes different feature images that all
encode local geometry as a scalar value, gaussian curvature, mean
curvate and flexion as described above. These feature images are
compared to bearing angles. Flexion images are expected to perform the best
as they give the best visual structure.

If the proposed localization pipeline does work as wished it gives a new
intermodal approach to localization and visual odometry.

Futhermore, it allows to apply algorithms and approaches from the
classical visual feature-world to depth sensors that became widely
available in robotic operations.

Und so weiter.

% \input{verzeichnisse/bezeichnungen.tex}
% \newpage
%
% \input{einfuehrung/content.tex}
% \newpage
% \input{grundlagen/content.tex}
% \newpage
% \input{modelle/content.tex}
% \newpage
% \input{intrinsic/content.tex}
% \newpage
% \input{extrinsic/content.tex}
% \newpage
% \input{evaluierung/content.tex}
% \newpage
% \input{fazit/content.tex}
% \newpage

\begin{appendix}
    % \appendix
    % \input{anhang/content}
    \newpage

    \addcontentsline{toc}{section}{\bibname}
    \bibliographystyle{IEEEtran}
    \bibliography{references}

    \newpage
    \addcontentsline{toc}{section}{\listtablename}\listoftables

    \newpage
    \addcontentsline{toc}{section}{\listfigurename}\listoffigures
    % \newpage

    %\renewcommand{\indexname}{Stichwortverzeichnis}
    %\addcontentsline{toc}{section}{Stichwortverzeichnis}
    % \printindex
\end{appendix}

\end{document}
